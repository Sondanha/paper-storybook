[
  {
    "scene_id": 1,
    "title": "기존 sequence transduction 모델의 한계",
    "narration": "기존의 주요 sequence transduction 모델들은 복잡한 순환신경망(RNN) 또는 합성곱신경망(CNN)에 기반하고 있습니다. 이러한 모델들은 인코더-디코더 구조와 어텐션 메커니즘을 사용하지만, 순차적 계산으로 인한 병렬화 어려움과 긴 학습 시간의 문제가 있습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "layout": "dot",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "viz_label": "auto_fallback",
        "diagram": "digraph G {\n                    node [shape=box, fontname=\"NanumGothic\", fontsize=12];\n                    \"기존 sequence trans...\" -> \"다음 단계\";\n                    }",
        "layout": "dot"
      }
    ]
  },
  {
    "scene_id": 2,
    "title": "Transformer 모델 제안",
    "narration": "이 논문에서는 순환신경망과 합성곱 신경망을 완전히 배제하고, 어텐션 메커니즘만을 이용한 새로운 신경망 구조인 'Transformer'를 제안합니다. 실험 결과, Transformer 모델은 기존 최고 성능 모델들을 능가하는 동시에 더 빠른 학습 속도를 보였습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "graph {\\n  fontname=\\\"NanumGothic\\\"\\n  fontsize=12\\n\\n  subgraph cluster_model {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델 구조</FONT>\\n    >;\\n    color=lightgrey;\\n    style=filled;\\n\\n    node1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Attention<BR/>Layer</FONT>\\n    >];\\n    node2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Feed-Forward<BR/>Network</FONT>\\n    >];\\n    node3 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Output</FONT>\\n    >];\\n\\n    node1 -- node2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">피드포워드<BR/>네트워크</FONT>\\n    >];\\n    node2 -- node3 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">출력</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_advantage {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer의 장점</FONT>\\n    >;\\n    color=lightgrey;\\n    style=filled;\\n\\n    node4 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">높은 성능</FONT>\\n    >];\\n    node5 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">빠른 학습 속도</FONT>\\n    >];\\n\\n    node4 -- node5 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">실험 결과</FONT>\\n    >];\\n  }\\n\\n  node1 -- node4 [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">어텐션 메커니즘</FONT>\\n  >];\\n  node2 -- node5 [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">학습 속도</FONT>\\n  >];\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "neato",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ngraph {\\n  fontname=\\\"NanumGothic\\\"\\n  fontsize=12\\n\\n  subgraph cluster_model {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델 구조</FONT>\\n    >;\\n    color=lightgrey;\\n    style=filled;\\n\\n    node1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Attention<BR/>Layer</FONT>\\n    >];\\n    node2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Feed-Forward<BR/>Network</FONT>\\n    >];\\n    node3 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Output</FONT>\\n    >];\\n\\n    node1 -- node2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">피드포워드<BR/>네트워크</FONT>\\n    >];\\n    node2 -- node3 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">출력</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_advantage {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer의 장점</FONT>\\n    >;\\n    color=lightgrey;\\n    style=filled;\\n\\n    node4 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">높은 성능</FONT>\\n    >];\\n    node5 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">빠른 학습 속도</FONT>\\n    >];\\n\\n    node4 -- node5 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">실험 결과</FONT>\\n    >];\\n  }\\n\\n  node1 -- node4 [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">어텐션 메커니즘</FONT>\\n  >];\\n  node2 -- node5 [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">학습 속도</FONT>\\n  >];\\n}"
      }
    ]
  },
  {
    "scene_id": 3,
    "title": "번역 과제 실험 결과",
    "narration": "Transformer 모델을 영어-독일어, 영어-프랑스어 기계번역 과제에 적용한 결과, 기존 최고 성능 모델들을 BLEU 점수 면에서 능가하였습니다. 특히 영어-프랑스어 과제에서는 단일 모델 기준 새로운 최고 성능을 달성했습니다.",
    "error": "JSON parse failed",
    "raw": "{\n  \"scene_id\": 3,\n  \"title\": \"번역 과제 실험 결과\",\n  \"narration\": \"Transformer 모델을 영어-독일어, 영어-프랑스어 기계번역 과제에 적용한 결과, 기존 최고 성능 모델들을 BLEU 점수 면에서 능가하였습니다. 특히 영어-프랑스어 과제에서는 단일 모델 기준 새로운 최고 성능을 달성했습니다.\",\n  \"viz_type\": \"diagram\",\n  \"tool\": \"graph {\\\\n  layout=twopi;\\\\n  rankdir=LR;\\\\n  node [fontname=\\\\\\\"NanumGothic\\\\\\\", fontsize=12];\\\\n  edge [fontname=\\\\\\\"NanumGothic\\\\\\\", fontsize=12];\\\\n\\\\n  node1 [label=<\\\\n    <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">영어-독일어</FONT>\\\\n  >];\\\\n  node2 [label=<\\\\n    <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">영어-프랑스어</FONT>\\\\n  >];\\\\n\\\\n  node3 [label=<\\\\n    <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">기존 최고 성능 모델</FONT>\\\\n  >];\\\\n  node4 [label=<\\\\n    <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">BLEU 점수 향상</FONT>\\\\n  >];\\\\n  node5 [label=<\\\\n    <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">새로운 최고 성능 달성</FONT>\\\\n  >];\\\\n\\\\n  nod"
  },
  {
    "scene_id": 4,
    "title": "다른 과제로의 일반화",
    "narration": "Transformer 모델은 기계번역 외에도 다른 과제에 적용될 수 있음을 보였습니다. 영어 문장 구조 분석 과제에서도 Transformer 모델은 큰 훈련 데이터와 적은 훈련 데이터 모두에서 우수한 성능을 보였습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "graph {\\n  layout=neato;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_transformer {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>\\n    >;\\n    node_transformer [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_tasks {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">다른 과제로의 일반화</FONT>\\n    >;\\n    node_tasks1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">기계번역</FONT>\\n    >];\\n    node_tasks2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">영어 문장 구조 분석</FONT>\\n    >];\\n  }\\n\\n  node_transformer -- node_tasks1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">적용</FONT>\\n  >];\\n  node_transformer -- node_tasks2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">적용</FONT>\\n  >];\\n\\n  subgraph cluster_data {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">훈련 데이터 크기</FONT>\\n    >;\\n    node_data1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">큰 데이터</FONT>\\n    >];\\n    node_data2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">적은 데이터</FONT>\\n    >];\\n  }\\n\\n  node_tasks2 -- node_data1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">우수한 성능</FONT>\\n  >];\\n  node_tasks2 -- node_data2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">우수한 성능</FONT>\\n  >];\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "circo",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ngraph {\\n  layout=neato;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_transformer {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>\\n    >;\\n    node_transformer [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_tasks {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">다른 과제로의 일반화</FONT>\\n    >;\\n    node_tasks1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">기계번역</FONT>\\n    >];\\n    node_tasks2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">영어 문장 구조 분석</FONT>\\n    >];\\n  }\\n\\n  node_transformer -- node_tasks1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">적용</FONT>\\n  >];\\n  node_transformer -- node_tasks2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">적용</FONT>\\n  >];\\n\\n  subgraph cluster_data {\\n    label=<\\n      <FONT FACE=\\\"NanumGothic\\\">훈련 데이터 크기</FONT>\\n    >;\\n    node_data1 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">큰 데이터</FONT>\\n    >];\\n    node_data2 [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">적은 데이터</FONT>\\n    >];\\n  }\\n\\n  node_tasks2 -- node_data1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">우수한 성능</FONT>\\n  >];\\n  node_tasks2 -- node_data2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">우수한 성능</FONT>\\n  >];\\n}"
      }
    ]
  },
  {
    "scene_id": 5,
    "title": "순환신경망과 Transformer의 차이",
    "narration": "기존 순환신경망 모델은 입출력 시퀀스의 위치를 계산 시간 단계에 맞춰 순차적으로 처리합니다. 이로 인해 병렬화가 어렵고 긴 시퀀스 길이에서 메모리 제약이 발생합니다. 반면 Transformer는 어텐션 메커니즘만을 사용하여 입출력 간 의존성을 모델링하므로 이러한 문제를 해결할 수 있습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "layout": "dot",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "viz_label": "auto_fallback",
        "diagram": "digraph G {\n                    node [shape=box, fontname=\"NanumGothic\", fontsize=12];\n                    \"순환신경망과 Transforme...\" -> \"다음 단계\";\n                    }",
        "layout": "dot"
      }
    ]
  },
  {
    "scene_id": 6,
    "title": "어텐션 메커니즘의 중요성",
    "narration": "어텐션 메커니즘은 기존 sequence transduction 모델에서 중요한 역할을 해왔습니다. 어텐션을 통해 입출력 간 거리에 관계없이 의존성을 모델링할 수 있습니다. 하지만 대부분의 경우 어텐션은 순환신경망과 함께 사용되어 왔습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "graph {\\n  layout=twopi;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  node1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Sequence Transduction Model</FONT>\\n  >];\\n  node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Attention Mechanism</FONT>\\n  >];\\n  node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Recurrent Network</FONT>\\n  >];\\n\\n  node1 -- node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Allows modeling of dependencies\\nwithout regard to distance in\\ninput or output sequences</FONT>\\n  >];\\n  node2 -- node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Used in conjunction with\\nin most cases</FONT>\\n  >];\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "twopi",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ngraph {\\n  layout=twopi;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  node1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Sequence Transduction Model</FONT>\\n  >];\\n  node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Attention Mechanism</FONT>\\n  >];\\n  node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Recurrent Network</FONT>\\n  >];\\n\\n  node1 -- node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Allows modeling of dependencies\\nwithout regard to distance in\\ninput or output sequences</FONT>\\n  >];\\n  node2 -- node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">Used in conjunction with\\nin most cases</FONT>\\n  >];\\n}"
      }
    ]
  },
  {
    "scene_id": 7,
    "title": "Transformer 모델의 구조",
    "narration": "Transformer 모델은 순환신경망과 합성곱 신경망을 완전히 배제하고, 오직 어텐션 메커니즘만을 이용하여 구성됩니다. 이를 통해 병렬화가 용이하고 학습 시간을 단축할 수 있습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "digraph Transformer {\\n  rankdir=TB;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_input {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">입력 데이터</FONT>>;\\n    input [label=<<FONT FACE=\\\"NanumGothic\\\">입력 시퀀스</FONT>>];\\n  }\\n\\n  subgraph cluster_encoder {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Encoder</FONT>>;\\n    encoder_layer1 [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer 1</FONT>>];\\n    encoder_layer2 [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer 2</FONT>>];\\n    encoder_layerN [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer N</FONT>>];\\n  }\\n\\n  subgraph cluster_decoder {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Decoder</FONT>>;\\n    decoder_layer1 [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer 1</FONT>>];\\n    decoder_layer2 [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer 2</FONT>>];\\n    decoder_layerN [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer N</FONT>>];\\n  }\\n\\n  subgraph cluster_output {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">출력 데이터</FONT>>;\\n    output [label=<<FONT FACE=\\\"NanumGothic\\\">출력 시퀀스</FONT>>];\\n  }\\n\\n  input -> encoder_layer1 -> encoder_layer2 -> ... -> encoder_layerN;\\n  encoder_layerN -> decoder_layer1 -> decoder_layer2 -> ... -> decoder_layerN -> output;\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "dot",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ndigraph Transformer {\\n  rankdir=TB;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_input {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">입력 데이터</FONT>>;\\n    input [label=<<FONT FACE=\\\"NanumGothic\\\">입력 시퀀스</FONT>>];\\n  }\\n\\n  subgraph cluster_encoder {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Encoder</FONT>>;\\n    encoder_layer1 [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer 1</FONT>>];\\n    encoder_layer2 [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer 2</FONT>>];\\n    encoder_layerN [label=<<FONT FACE=\\\"NanumGothic\\\">Encoder<BR/>Layer N</FONT>>];\\n  }\\n\\n  subgraph cluster_decoder {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Decoder</FONT>>;\\n    decoder_layer1 [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer 1</FONT>>];\\n    decoder_layer2 [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer 2</FONT>>];\\n    decoder_layerN [label=<<FONT FACE=\\\"NanumGothic\\\">Decoder<BR/>Layer N</FONT>>];\\n  }\\n\\n  subgraph cluster_output {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">출력 데이터</FONT>>;\\n    output [label=<<FONT FACE=\\\"NanumGothic\\\">출력 시퀀스</FONT>>];\\n  }\\n\\n  input -> encoder_layer1 -> encoder_layer2 -> ... -> encoder_layerN;\\n  encoder_layerN -> decoder_layer1 -> decoder_layer2 -> ... -> decoder_layerN -> output;\\n}"
      }
    ]
  },
  {
    "scene_id": 8,
    "title": "Transformer 모델의 우수성",
    "narration": "실험 결과, Transformer 모델은 기존 최고 성능 모델들을 능가하는 우수한 성능을 보였습니다. 특히 병렬화가 가능하여 훨씬 빠른 학습 속도를 달성할 수 있었습니다.",
    "error": "JSON parse failed",
    "raw": "{\n  \"scene_id\": 8,\n  \"title\": \"Transformer 모델의 우수성\",\n  \"narration\": \"실험 결과, Transformer 모델은 기존 최고 성능 모델들을 능가하는 우수한 성능을 보였습니다. 특히 병렬화가 가능하여 훨씬 빠른 학습 속도를 달성할 수 있었습니다.\",\n  \"viz_type\": \"diagram\",\n  \"tool\": \"graph {\\\\n  node [fontname=\\\\\\\"NanumGothic\\\\\\\", fontsize=12];\\\\n  edge [fontname=\\\\\\\"NanumGothic\\\\\\\", fontsize=12];\\\\n\\\\n  subgraph cluster_models {\\\\n    label=<\\\\n      <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">기존 최고 성능 모델</FONT>\\\\n    >;\\\\n    style=filled;\\\\n    color=lightgrey;\\\\n    node [style=filled,color=white];\\\\n    \\\\\\\"Model1\\\\\\\" [label=<\\\\n      <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">모델 1</FONT>\\\\n    >];\\\\n    \\\\\\\"Model2\\\\\\\" [label=<\\\\n      <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">모델 2</FONT>\\\\n    >];\\\\n    \\\\\\\"Model3\\\\\\\" [label=<\\\\n      <FONT FACE=\\\\\\\"NanumGothic\\\\\\\">모델 3</FONT>\\\\n    >];\\\\n  }\\\\n\\\\n "
  },
  {
    "scene_id": 9,
    "title": "영어-독일어 번역 과제 결과",
    "narration": "WMT 2014 영어-독일어 번역 과제에서 TRANSFORMER (Transformer) 모델은 기존 최고 결과 대비 2 BLEU 포인트 이상 향상된 28.4 BLEU 점수를 달성했습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "digraph G {\\n  rankdir=LR;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  node1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">WMT 2014<br/>영어-독일어 번역 과제</FONT>\\n  >];\\n  node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">TRANSFORMER<br/>모델</FONT>\\n  >];\\n  node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">28.4 BLEU<br/>점수</FONT>\\n  >];\\n  node4 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">기존 최고<br/>결과 대비</FONT>\\n  >];\\n  node5 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">2 BLEU<br/>포인트 향상</FONT>\\n  >];\\n\\n  node1 -> node2 -> node3 -> node4 -> node5;\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "dot",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ndigraph G {\\n  rankdir=LR;\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  node1 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">WMT 2014<br/>영어-독일어 번역 과제</FONT>\\n  >];\\n  node2 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">TRANSFORMER<br/>모델</FONT>\\n  >];\\n  node3 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">28.4 BLEU<br/>점수</FONT>\\n  >];\\n  node4 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">기존 최고<br/>결과 대비</FONT>\\n  >];\\n  node5 [label=<\\n    <FONT FACE=\\\"NanumGothic\\\">2 BLEU<br/>포인트 향상</FONT>\\n  >];\\n\\n  node1 -> node2 -> node3 -> node4 -> node5;\\n}"
      }
    ]
  },
  {
    "scene_id": 10,
    "title": "영어-프랑스어 번역 과제 결과",
    "narration": "WMT 2014 영어-프랑스어 번역 과제에서 Transformer 모델은 단일 모델 기준 새로운 최고 성능인 41.8 BLEU 점수를 달성했습니다. 이는 기존 최고 모델들에 비해 훨씬 적은 학습 비용으로 달성한 결과입니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "layout": "dot",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "viz_label": "auto_fallback",
        "diagram": "digraph G {\n                    node [shape=box, fontname=\"NanumGothic\", fontsize=12];\n                    \"영어-프랑스어 번역 과제 결과\" -> \"다음 단계\";\n                    }",
        "layout": "dot"
      }
    ]
  },
  {
    "scene_id": 11,
    "title": "Transformer의 일반화 능력",
    "narration": "Transformer 모델은 기계번역 외에도 영어 문장 구조 분석 과제에서도 우수한 성능을 보였습니다. 이는 Transformer가 다양한 과제에 일반화될 수 있음을 시사합니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "graph {\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_0 {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델의 일반화 능력</FONT>>;\\n    style=filled;\\n    color=lightgrey;\\n\\n    machine_translation [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">기계번역</FONT>\\n    >];\\n    parsing [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">문장 구조 분석</FONT>\\n    >];\\n\\n    machine_translation -- parsing [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델이 우수한 성능을 보임</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_1 {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">이를 통해 알 수 있는 점</FONT>>;\\n    style=filled;\\n    color=lightgrey;\\n\\n    generalization [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer가 다양한 과제에 일반화될 수 있음</FONT>\\n    >];\\n  }\\n\\n  machine_translation -- generalization [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">제시</FONT>\\n  >];\\n  parsing -- generalization [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">제시</FONT>\\n  >];\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "dot",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ngraph {\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_0 {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델의 일반화 능력</FONT>>;\\n    style=filled;\\n    color=lightgrey;\\n\\n    machine_translation [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">기계번역</FONT>\\n    >];\\n    parsing [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">문장 구조 분석</FONT>\\n    >];\\n\\n    machine_translation -- parsing [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer 모델이 우수한 성능을 보임</FONT>\\n    >];\\n  }\\n\\n  subgraph cluster_1 {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">이를 통해 알 수 있는 점</FONT>>;\\n    style=filled;\\n    color=lightgrey;\\n\\n    generalization [label=<\\n      <FONT FACE=\\\"NanumGothic\\\">Transformer가 다양한 과제에 일반화될 수 있음</FONT>\\n    >];\\n  }\\n\\n  machine_translation -- generalization [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">제시</FONT>\\n  >];\\n  parsing -- generalization [style=dashed, label=<\\n    <FONT FACE=\\\"NanumGothic\\\">제시</FONT>\\n  >];\\n}"
      }
    ]
  },
  {
    "scene_id": 12,
    "title": "결론",
    "narration": "이 연구에서는 순환신경망과 합성곱 신경망의 한계를 극복하고자 어텐션 메커니즘만을 이용한 Transformer 모델을 제안했습니다. Transformer 모델은 기존 최고 성능 모델들을 능가하는 우수한 결과를 보였으며, 더 빠른 학습 속도와 뛰어난 일반화 능력을 보였습니다.",
    "viz_type": "diagram",
    "tool": "graphviz",
    "diagram": "graph {\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_model {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>>;\\n    node1 [label=<<FONT FACE=\\\"NanumGothic\\\">Attention 메커니즘</FONT>>];\\n    node2 [label=<<FONT FACE=\\\"NanumGothic\\\">순환신경망/합성곱 신경망<BR/>한계 극복</FONT>>];\\n    node3 [label=<<FONT FACE=\\\"NanumGothic\\\">높은 성능<BR/>빠른 학습<BR/>우수한 일반화</FONT>>];\\n    edge [color=blue];\\n    node1 -- node2;\\n    node2 -- node3;\\n  }\\n\\n  subgraph cluster_compare {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">기존 최고 모델과 비교</FONT>>;\\n    node4 [label=<<FONT FACE=\\\"NanumGothic\\\">기존 최고 모델</FONT>>];\\n    node5 [label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>>];\\n    edge [color=red];\\n    node4 -- node5 [label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델이 성능 우수</FONT>>];\\n  }\\n}",
    "visualizations": [
      {
        "viz_type": "diagram",
        "tool": "graphviz",
        "layout": "dot",
        "viz_label": "auto_primary",
        "diagram": "graph [fontname=\"NanumGothic\", fontsize=12];\nnode [fontname=\"NanumGothic\", fontsize=12];\nedge [fontname=\"NanumGothic\", fontsize=12];\ngraph {\\n  node [fontname=\\\"NanumGothic\\\", fontsize=12];\\n  edge [fontname=\\\"NanumGothic\\\", fontsize=12];\\n\\n  subgraph cluster_model {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>>;\\n    node1 [label=<<FONT FACE=\\\"NanumGothic\\\">Attention 메커니즘</FONT>>];\\n    node2 [label=<<FONT FACE=\\\"NanumGothic\\\">순환신경망/합성곱 신경망<BR/>한계 극복</FONT>>];\\n    node3 [label=<<FONT FACE=\\\"NanumGothic\\\">높은 성능<BR/>빠른 학습<BR/>우수한 일반화</FONT>>];\\n    edge [color=blue];\\n    node1 -- node2;\\n    node2 -- node3;\\n  }\\n\\n  subgraph cluster_compare {\\n    label=<<FONT FACE=\\\"NanumGothic\\\">기존 최고 모델과 비교</FONT>>;\\n    node4 [label=<<FONT FACE=\\\"NanumGothic\\\">기존 최고 모델</FONT>>];\\n    node5 [label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델</FONT>>];\\n    edge [color=red];\\n    node4 -- node5 [label=<<FONT FACE=\\\"NanumGothic\\\">Transformer 모델이 성능 우수</FONT>>];\\n  }\\n}"
      }
    ]
  }
]